configfile: "../config/config.yaml"
localrules: all, get_read_groups
# update config based on secondary configuration file
import os
from yaml import safe_load
with open("../config/project.yaml") as project_fh:
	config.update(safe_load(project_fh))

reference_data = config["reference"][config["genome_build"]]

def get_samples(samples_file="samples.list"):
	with open(samples_file) as samples_fh:
		return [sample.strip() for sample in samples_fh.read().splitlines()]

wildcard_constraints:
	sample="\d{10}(?:_ds)?", # samples are exclusively labeled as 10-digit integers so far
	rg="\d+", # samtools labels as integers starting from 0
	group="\d+"

def bams_by_read_group(wildcards):
	read_groups = []
	with open("../scratch/{sample}/{sample}.rgs".format(
		sample=wildcards["sample"])) as rgs_fh:
		for line in rgs_fh:
			read_groups.append(line.split("\t")[1].split(":")[1])
	return ["../scratch/{sample}/{sample}_{rg}.bam".format(
		sample=wildcards["sample"], rg=read_group)
		for read_group in read_groups]

rule get_read_groups:
	"""Extract the set of read groups from a BAM's header."""
	input:
		"../input/bams/{sample}.bam"
	output:
		"../scratch/{sample}/{sample}.rgs"
	conda:
		"envs/bwa_samtools.yaml"
	shell:
		"samtools view -H {input} | grep '^@RG' > {output}"

checkpoint rule split_bam:
	"""Split a BAM by read group."""
	input:
		bam = "../input/bams/{sample}.bam",
		rgs = "../scratch/{sample}/{sample}.rgs"
	output:
		touch("../scratch/{sample}/{sample}.split_bam.done")
	params:
		stdout = "../logs/split_bam/{sample}.out",
		stderr = "../logs/split_bam/{sample}.err"
	conda:
		"envs/bwa_samtools.yaml"
	shell:
		"samtools split -f ../scratch/{wildcards.sample}/"
		"{wildcards.sample}_%#.bam {input.bam}"

rule extract_fastq_from_bam:
	"""Extract the reads from a BAM into an interleaved FASTQ."""
	input:
		flag = "../scratch/{sample}/{sample}.split_bam.done",
		bam = "../scratch/{sample}/{sample}_{rg}.bam"
	output:
		temp("../scratch/{sample}/{sample}_{rg}.fq")
	params:
		stdout = "../logs/extract_fastqs/{sample}_{rg}.out",
		stderr = "../logs/extract_fastqs/{sample}_{rg}.err"
	conda:
		"envs/bwa_samtools.yaml"
	shell:
		"samtools fastq -0 /dev/null {input.bam} > {output}"

rule bwa_align_and_sort:
	"""Align a FASTQ of interleaved reads."""
	input:
		fq = "../scratch/{sample}/{sample}_{rg}.fq",
		rgs = "../scratch/{sample}/{sample}.rgs"
	output:
		temp("../scratch/{sample}/{sample}_reprocessed_{rg}.bam")
	threads: 8
	params:
		rg = lambda wildcards: int(wildcards.rg) + 1,
		stdout = "../logs/bwa/{sample}_{rg}.out",
		stderr = "../logs/bwa/{sample}_{rg}.err"
	conda:
		"envs/bwa_samtools.yaml"
	shell:
		"RG=$(sed -n '{params.rg} s:\\t:\\\\t:gp' < {input.rgs}) && "
		"bwa mem "
		"-K 100000000 " # suggested for reproducibility
		"-p " # interleaved reads
		"-Y " # soft clipping of supplementary alignments
		"-t {threads} "
		'-R "$RG" '
		"{reference_data[genome]} "
		"{input.fq} "
		"-Y "
		"| samtools sort -T $TMPDIR "
		"--threads {threads} -O BAM -o {output} -"

def aggregate_read_groups(wildcards):
	split_bams_directory = os.path.dirname(
			checkpoints.split_bam.get(**wildcards).output[0])
	return expand("../scratch/{sample}/{sample}_reprocessed_{rg}.bam",
			sample=wildcards.sample,
			rg=glob_wildcards(
				"{split_bams_directory}/{sample}_{{rg,\d+}}.bam".format(
					split_bams_directory=split_bams_directory,
					sample=wildcards.sample)).rg)

rule mark_duplicates:
	"""Mark PCR duplicates and merge per-RG BAMs."""
	input:
		aggregate_read_groups
	output:
		bam = temp("../scratch/{sample}/{sample}_markdup.bam"),
		metrics = "../output/qc/{sample}_metrics.txt"
	params:
		inputs = lambda wildcards, input: "--INPUT " + " --INPUT ".join(input),
		memory_min = "20G",
		memory_max = "24G", #jvm
		stdout = "../logs/mark_duplicates/{sample}.out",
		stderr = "../logs/mark_duplicates/{sample}.err"
	resources:
		mem = "32gb" #cluster
	conda:
		"envs/picard.yaml"
	shell:
		"picard MarkDuplicates "
		"-Djava.io.tmpdir=$TMPDIR -Xms{params.memory_min} -Xmx{params.memory_max} "
		"{params.inputs} "
		"--OUTPUT {output.bam} "
		"--METRICS_FILE {output.metrics} "
		"--OPTICAL_DUPLICATE_PIXEL_DISTANCE 2500 "
		"--VALIDATION_STRINGENCY SILENT"

rule base_recalibration:
	input:
		"../scratch/{sample}/{sample}_markdup.bam"
	output:
		"../output/recal_tables/{sample}_recal.table"
	params:
		memory = "24G", #jvm
		stdout = "../logs/base_recalibration/{sample}.out",
		stderr = "../logs/base_recalibration/{sample}.err"
	resources:
		mem = "32gb" #cluster
	conda:
		"envs/gatk.yaml"
	shell:
		"gatk "
		'--java-options "-Djava.io.tmpdir=$TMPDIR -Xms{params.memory}" '
		"BaseRecalibrator "
		"-R {reference_data[genome]} "
		"-I {input} "
		"--known-sites {reference_data[dbsnp]} "
		"--known-sites {reference_data[indel_mills]} "
		"--known-sites {reference_data[indel_known]} "
		"-O {output}"

rule apply_BQSR:
	input:
		bam = "../scratch/{sample}/{sample}_markdup.bam",
		recal_table = "../output/recal_tables/{sample}_recal.table"
	output:
		bam = "../output/bams/{sample}.bam",
		bai = "../output/bams/{sample}.bai"
	params:
		memory = "24G", #jvm
		stdout = "../logs/apply_BQSR/{sample}.out",
		stderr = "../logs/apply_BQSR/{sample}.err"
	resources:
		mem = "32gb", #cluster
		mail = "ae" # send an email when finished as this is the last step
	conda:
		"envs/gatk.yaml"
	shell:
		"gatk "
		'--java-options "-Djava.io.tmpdir=$TMPDIR -Xms{params.memory}" '
		"ApplyBQSR "
		"-R {reference_data[genome]} "
		"-I {input.bam} "
		"-O {output.bam} "
		"-bqsr {input.recal_table} "
		"--static-quantized-quals 10 "
		"--static-quantized-quals 20 "
		"--static-quantized-quals 30 "
		"--add-output-sam-program-record "
		"--create-output-bam-md5 "

rule haplotype_caller:
	input:
		bam = "../output/bams/{sample}.bam",
		interval_file = "../resources/intervals/intervalfile_{group}.list" # parallelization - currently 120x
	output:
		"../scratch/{sample}/gvcfs/{sample}_{group}.g.vcf.gz"
	params:
		memory = "6G",
		stdout = "../logs/haplotype_caller/{sample}_{group}.out",
		stderr = "../logs/haplotype_caller/{sample}_{group}.err"
	resources:
		mem = "12gb"
	conda:
		"envs/gatk.yaml"
	shell:
		"gatk "
		'--java-options "-Djava.io.tmpdir=$TMPDIR -Xms{params.memory}" '
		"HaplotypeCaller "
		"-R {reference_data[genome]} "
		"-I {input.bam} "
		"-L {input.interval_file} "
		"-O {output} "
		"-GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 "
		"-GQB 60 -GQB 70 -GQB 80 -GQB 90 "
		"-ERC GVCF "
		"-G StandardAnnotation "
		"-G AS_StandardAnnotation "
		"-G StandardHCAnnotation "

rule merge_gvcfs:
	input:
		expand("../scratch/{{sample}}/gvcfs/{{sample}}_{group}.g.vcf.gz",
				group=sorted(
					[int(value) for value in glob_wildcards(
					"../resources/intervals/intervalfile_{group,\d+}.list").group]))
	output:
		"../output/gvcfs/{sample}.g.vcf.gz"
	params:
		inputs = lambda wildcards, input: "--INPUT " + " --INPUT ".join(input),
		memory = "12G",
		stdout = "../logs/merge_gvcfs/{sample}.out",
		stderr = "../logs/merge_gvcfs/{sample}.err"
	resources:
		mail = "ae"
	conda:
		"envs/picard.yaml"
	shell:
		"picard MergeVcfs "
		"-Djava.io.tmpdir=$TMPDIR -Xmx{params.memory} "
		"{params.inputs} "
		"--OUTPUT {output}"

rule genomics_db_import:
	input:
		gvcfs = expand("../scratch/{sample}/gvcfs/{sample}_{{group}}.g.vcf.gz",
				sample=get_samples()),
		interval_file = "../resources/intervals/intervalfile_{group}.list"
	output:
		directory("../scratch/genomics_db_{group}")
	params:
		inputs = lambda wildcards, input: "-V " + " -V ".join(input.gvcfs),
		stdout = "../logs/genomics_db_import/{group}.out",
		stderr = "../logs/genomics_db_import/{group}.err"
	conda:
		"envs/gatk.yaml"
	shell:
		"gatk "
		'--java-options "-Djava.io.tmpdir=$TMPDIR" '
		"GenomicsDBImport "
		"{params.inputs} "
		"--genomicsdb-workspace-path {output} "
		"--tmp-dir $TMPDIR "
		"-L {input.interval_file}"

rule genotype_gvcfs:
	input:
		genomics_db = "../scratch/genomics_db_{group}",
		interval_file = "../resources/intervals/intervalfile_{group}.list"
	output:
		temp("../scratch/vcfs/{group}.vcf")
	params:
		stdout = "../logs/genotype_gvcfs/{group}.out",
		stderr = "../logs/genotype_gvcfs/{group}.err"
	conda:
		"envs/gatk.yaml"
	shell:
		"gatk "
		'--java-options "-Djava.io.tmpdir=$TMPDIR" '
		"GenotypeGVCFs "
		"-R {reference_data[genome]} "
		"-V gendb://{input.genomics_db} "
		"-D {reference_data[dbsnp]} "
		"-G StandardAnnotation "
		"-G AS_StandardAnnotation "
		"-G StandardHCAnnotation "
		"-L {input.interval_file} "
		"--only-output-calls-starting-in-intervals "
		"-O {output}"

rule snp_recalibration:
	input:
		vcf = "../scratch/vcfs/{group}.vcf",
		interval_file = "../resources/intervals/intervalfile_{group}.list"
	output:
		recal = temp("../scratch/recal/{group}_snps.recal"),
		tranches = temp("../scratch/recal/{group}_snps.tranches")
	params:
		stdout = "../logs/snp_recalibration/{group}.out",
		stderr = "../logs/snp_recalibration/{group}.err"
	conda:
		"envs/gatk.yaml"
	shell:
		"gatk "
		'--java-options "-Djava.io.tmpdir=$TMPDIR" '
		"VariantRecalibrator "
		"-V {input.vcf} "
		"-R {reference_data[genome]} "
		"--resource hapmap,known=false,training=true,truth=true,prior=15.0:{reference_data[hapmap]} "
		"--resource omni,known=false,training=true,truth=false,prior=12.0:{reference_data[omni]} "
		"--resource 1000G,known=false,training=true,truth=false,prior=10.0:{reference_data[onekg]} "
		"--resource dbsnp,known=true,training=false,truth=false,prior=2.0:{reference_data[dbsnp]} "
		"-an QD -an MQ -an MQRankSum -an ReadPosRankSum -an SOR -an DP "
		"-mode SNP -AS "
		"-O {output.recal} "
		"--tranches-file {output.tranches}"

rule apply_snp_recalibration:
	input:
		vcf = "../scratch/vcfs/{group}.vcf",
		recal = "../scratch/recal/{group}_snps.recal",
		tranches = "../scratch/recal/{group}_snps.tranches",
	output:
		temp("../scratch/vcfs/{group}_recal_snp.vcf")
	params:
		stdout = "../logs/apply_snp_recalibration/{group}.out",
		stderr = "../logs/apply_snp_recalibration/{group}.err"
	conda:
		"envs/gatk.yaml"
	shell:
		"gatk "
		'--java-options "-Djava.io.tmpdir=$TMPDIR" '
		"ApplyVQSR "
		"-R {reference_data[genome]} "
		"-V {input.vcf} "
		"--recal-file {input.recal} "
		"--tranches-file {input.tranches} "
		"--mode SNP -AS "
		"-O {output}"

rule indel_recalibration:
	input:
		vcf = "../scratch/vcfs/{group}_recal_snp.vcf"
	output:
		recal = temp("../scratch/recal/{group}_indels.recal"),
		tranches = temp("../scratch/recal/{group}_indels.tranches")
	params:
		stdout = "../logs/indel_recalibration/{group}.out",
		stderr = "../logs/indel_recalibration/{group}.err"
	conda:
		"envs/gatk.yaml"
	shell:
		"gatk "
		'--java-options "-Djava.io.tmpdir=$TMPDIR" '
		"VariantRecalibrator "
		"-V {input.vcf} "
		"--resource mills,known=false,training=true,truth=true,prior=12.0:{reference_data[indel_mills]} "
		"--resource "
		"-an QD -an MQRankSum -an ReadPosRankSum -an SOR -an DP -an FS "
		"-mode INDEL -AS --max-gaussians 4 "
		"-O {output.recal} "
		"--tranches-file {output.tranches} "
		"--resource mills,known=false,training=true,truth=true,prior=12.0:{reference_data[indel_mills]} "
		"--resource axiom,known=false,training=true,truth=false,prior=10.0:{reference_data[axiom]} "
		"--resource dbsnp,known=true,training=false,truth=false,prior=2.0:{reference_data[dbsnp]} "
		"-an QD -an MQ -an MQRankSum -an ReadPosRankSum -an SOR -an DP "
		"-mode SNP -AS "
		"-O {output.recal} "
		"--tranches-file {output.tranches}"

rule apply_indel_recalibration:
	input:
		vcf = "../scratch/vcfs/{group}_recal_snp.vcf",
		recal = "../scratch/recal/{group}_indels.recal",
		tranches = "../scratch/recal/{group}_indels.tranches"
	output:
		temp("../scratch/vcfs/{group}.recal.vcf")
	params:
		stdout = "../logs/apply_indel_recalibration/{group}.out",
		stderr = "../logs/apply_indel_recalibration/{group}.err"
	conda:
		"envs/gatk.yaml"
	shell:
		"gatk "
		'--java-options "-Djava.io.tmpdir=$TMPDIR" '
		"ApplyVQSR "
		"-R {reference_data[genome]} "
		"-V {input.vcf "
		"--recal-file {input.recal} "
		"--tranches-file {input.tranches} "
		"--mode INDEL -AS "
		"-O {output} "

rule merge_vcfs:
	input:
		expand("../scratch/vcfs/{group}.recal.vcf",
				group=sorted(
					[int(value) for value in glob_wildcards(
						"../resources/intervals/intervalfile_{group,\d+}.list").group]))
	output:
		"../output/joint_vcf/final.vcf.gz"
	params:
		inputs = lambda wildcards, input: "--INPUT " + " --INPUT ".join(input),
		stdout = "../logs/merge_vcfs/final.out",
		stderr = "../logs/merge_vcfs/final.err"
	resources:
		mail = "ae"
	conda:
		"envs/gatk.yaml"
	shell:
		"gatk "
		'--java-options "-Djava.io.tmpdir=$TMPDIR" '
		"GatherVcfs "
		"--TMP_DIR $TMPDIR "
		"{params.inputs} "
		"--OUTPUT {output}"
