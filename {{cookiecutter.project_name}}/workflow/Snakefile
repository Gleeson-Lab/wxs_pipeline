configfile: "../config/config.yaml"


localrules:
    all,
    get_read_groups,
    somalier_relate_read_groups,
    check_rgs_contamination,
    check_contamination,
    check_read_group_relatedness,
    copy_program_to_scratch,
    somalier_relate_samples,
    create_interval_files,
    check_relatedness,
    predict_sex,
    check_sex,


import os
import re
from functools import lru_cache, partial
from glob import glob
from utilities import get_options


genome_build = config["genome_build"]
reference_data = config["reference"][genome_build]
sample_regex = "[a-zA-Z\d_-]+"  # sample identifiers must match this
get_options = partial(get_options, config=config)


wildcard_constraints:
    sample=sample_regex,
    rg="\d+",  # samtools labels as integers starting from 0
    group="\d+",  # the genome interval subsets to restrict variant calling to so as to achieve parallelization


@lru_cache
def get_samples():
    """
    Returns a list of samples in ../input/bams and ../input/fastqs.
    This is used to control which samples are joint genotyped together.
    """
    samples = set()
    bam_regex = re.compile(f"(?P<sample>{sample_regex})\.bam")
    samples_not_matching_regex = []
    for bam in glob("../input/bams/*.bam"):
        bam = os.path.basename(bam)
        m = bam_regex.match(bam)
        if m:
            samples.add(m.groupdict()["sample"])
        else:
            samples_not_matching_regex.append(bam)
    fastq_regex = re.compile(f"(?P<sample>{sample_regex})_0\.R1\.fastq\.gz")
    for fastq in glob("../input/fastqs/*_0.R1.fastq.gz"):
        fastq = os.path.basename(fastq)
        m = fastq_regex.match(fastq)
        if m:
            samples.add(m.groupdict()["sample"])
        else:
            samples_not_matching_regex.append(fastq)
    if samples_not_matching_regex:
        raise ValueError(
            f"The following input files were found that do not "
            f"match the expected format: {samples_not_matching_regex}."
        )
    if samples:
        return sorted(samples)
    else:
        raise ValueError("Didn't find any samples in ../input.")


all_list = [
    "../output/qc/somalier/samples_with_predicted_sex.ped",
    expand("../output/gvcfs/{sample}.g.vcf.gz", sample=get_samples()),
    expand("../output/qc/mosdepth/{sample}.mosdepth.summary.txt", sample=get_samples()),
    expand("../output/qc/collectinsertsizemetrics/{sample}.txt", sample=get_samples()),
]


# if PED file is included, check relatedness also
PED_FILE = "../input/samples.ped"
if os.path.isfile(PED_FILE):
    all_list.append("../scratch/check_relatedness.done")
    all_list.append("../scratch/check_sex.done")


rule all:
    """
    Dummy rule to get:
    1. joint-called VCF
    2. single-sample GVCFs
    3. QC information
    """
    input:
        all_list,


def increment(wildcards, attempt, base, step):
    """
    Returns the walltime to run the given job for based on the number of
    attempts.
    """
    return base + step * (attempt - 1)


def get_output_directory(wildcards, output):
    """
    Return the output directory for the rule.
    """
    return os.path.dirname(output[0])


def bwa_options_interleaved(wildcards, input):
    """
    Return the extra options to pass to bwa for an interleaved FASTQ (for reprocessing BAMs).
    -p # interleaved reads
    """
    rg_number = int(wildcards.rg)
    rg = ""
    with open(input.rgs) as rgs_fh:
        for x, line in enumerate(rgs_fh):
            if x == rg_number:
                rg = line.strip().replace("\t", r"\t")
                break
    if not rg:
        raise ValueError(
            f"Could not find matching read group {rg_number} from {input.rgs}."
        )
    return get_options("bwa", prefix="-", extra=[("R", f"'{rg}'"), ("p", "")])


def bwa_options_paired_end(wildcards, input):
    """
    Return the extra options to pass to bwa for paired-end FASTQs.
    """
    rg_lines = []
    with open(input.rg) as rg_fh:
        for line in rg_fh:
            line = line.strip()
            if line:
                rg_lines.append(line)
    if len(rg_lines) != 1:
        raise ValueError(
            f"Found {len(rg_lines)} lines in RG file {input.rg} (should be 1)."
        )
    rg = rg_lines[0].replace("\t", r"\t")
    return get_options("bwa", prefix="-", extra=[("R", f"'{rg}'")])


def get_java_temp_directory(wildcards):
    """
    Return the directory specified by $TMPDIR if present, empty string otherwise.
    """
    tmp_dir = os.environ.get("TMPDIR", "")
    if tmp_dir:
        return f"-Djava.io.tmpdir={tmp_dir}"
    else:
        return ""


def aggregate_read_groups_markduplicates(wildcards):
    """
    Returns a list of all BAMs for the given sample, one per read group.
    """
    read_groups = aggregate_read_groups(sample=wildcards["sample"])
    if len(read_groups) > 1:
        # we do not use the output from checking somalier not verifybamid but poll
        # the checkpoint for these rules to enforce snakemake running these steps
        # and verifying that they ran properly
        # also these steps are unnecessary if there is only one read group
        checkpoints.check_rgs_contamination.get(**wildcards)
        checkpoints.check_read_group_relatedness.get(**wildcards)
    return expand(
        f"../scratch/{wildcards['sample']}/{wildcards['sample']}_{{rg}}.bam",
        rg=read_groups,
    )


def aggregate_read_groups(sample):
    """
    Returns a list of all read groups for the sample.
    If this function is run as part of an input function for a rule and the
    original file was a BAM, snakemake will infer it needs to run split_bam on it.
    """
    # need to handle case of BAM/FASTQs separately
    if os.path.isfile(f"../input/bams/{sample}.bam"):
        # N.B. we already know what this directory is, i.e. ../scratch/{sample},
        # but we must run checkpoints.split_bam.get() so that snakemake
        # infers the rule calling this is dependent upon split_bam
        split_bams_directory = os.path.dirname(
            checkpoints.split_bam.get(sample=sample).output[0]
        )
        fns = glob_wildcards(
            f"{split_bams_directory}/{sample}_original_{{rg,\d+}}.bam"
        ).rg
    else:
        # use as input the BAMs corresponding to each pair of FASTQs
        fns = glob_wildcards(f"../input/fastqs/{sample}_{{rg,\d+}}.R1.fastq.gz").rg
    if fns:
        return fns
    else:
        raise ValueError(
            f"Error in aggregate_read_groups finding read groups for {sample}."
        )


def get_final_bam(wildcards):
    """
    Returns the path to the final BAM for the sample while enforcing that it is
    insufficiently contaminated to continue.
    """
    checkpoints.check_contamination.get(**wildcards)
    return f"../output/bams/{wildcards['sample']}.bam"


def get_intervals():
    """
    Returns a sorted list of intervals for use in inferring what VCF/gVCFs should exist.
    """
    # need to make sure we actually created the inveral files first or the list will be empty
    checkpoints.create_interval_files.get()
    # sorting ensures we merge in the proper order, not random
    return sorted(
        [
            int(value)
            for value in glob_wildcards(
                "../resources/intervals/intervalfile_{group,\d+}.list"
            ).group
        ]
    )


rule get_read_groups:
    """
    Extract the set of read groups from a BAM's (or CRAM's) header.
    The SM attribute is updated to {sample}, but others are unchanged.
    """
    input:
        "../input/bams/{sample}.bam",
    output:
        "../scratch/{sample}/{sample}.rgs",
    resources:
        walltime=0,
    conda:
        "envs/samtools.yaml"
    shell:
        "samtools view -H {input} | grep '^@RG' | sed 's/SM:\S*/SM:{wildcards.sample}/' > {output}"


checkpoint split_bam:
    """
    Split a BAM (or CRAM) by read group.
    http://www.htslib.org/doc/samtools-split.html
    """
    input:
        bam="../input/bams/{sample}.bam",
        rgs="../scratch/{sample}/{sample}.rgs",
    output:
        touch("../scratch/{sample}/{sample}.split_bam.done"),
    resources:
        walltime=partial(increment, base=8, step=8),
    log:
        "../logs/split_bam/{sample}.log",
    group:
        "bam_to_fastq"
    conda:
        "envs/samtools.yaml"
    shell:
        "samtools split -f ../scratch/{wildcards.sample}/"
        "{wildcards.sample}_original_%#.bam {input.bam} > {log} 2>&1"


rule extract_fastq_from_bam:
    """
    Extract the reads from a BAM into an interleaved FASTQ.
    http://www.htslib.org/doc/samtools-fasta.html
    """
    input:
        flag="../scratch/{sample}/{sample}.split_bam.done",
        bam="../scratch/{sample}/{sample}_original_{rg}.bam",
    output:
        temp("../scratch/{sample}/{sample}_{rg}.fq"),
    resources:
        walltime=partial(increment, base=8, step=8),
    log:
        "../logs/extract_fastqs/{sample}_{rg}.log",
    group:
        "bam_to_fastq"
    conda:
        "envs/samtools.yaml"
    shell:
        "samtools fastq -0 /dev/null {input.bam} > {output} 2> {log}"


rule bwa_align_and_sort_interleaved:
    """
    Align a FASTQ of interleaved reads.
    http://bio-bwa.sourceforge.net/bwa.shtml
    """
    input:
        reads="../scratch/{sample}/{sample}_{rg}.fq",
        rgs="../scratch/{sample}/{sample}.rgs",
        # this is not actually used by the
        # wrapper (must be specified as params.index) this is here to ensure the
        # reference genome actually exists
        genome=reference_data["fasta"],
    output:
        temp("../scratch/{sample}/{sample}_{rg}.bam"),
    params:
        extra=bwa_options_interleaved,
        sorting="samtools",
        index=reference_data["fasta"],
    resources:
        walltime=partial(increment, base=16, step=8),
    threads: 8
    log:
        "../logs/bwa/{sample}_{rg}.log",
    wrapper:
        "0.78.0/bio/bwa/mem"


rule bwa_align_and_sort_paired_end:
    """
    Align a pair of FASTQs.
    This is the starting point of processing when beginning with FASTQs as opposed to a BAM file.
    http://bio-bwa.sourceforge.net/bwa.shtml
    """
    input:
        reads=[
            "../input/fastqs/{sample}_{rg}.R1.fastq.gz",
            "../input/fastqs/{sample}_{rg}.R2.fastq.gz",
        ],
        # this is one line that will be passed to bwa, for example,
        # @RG\tID:C4HFE.4\tDT:2014-06-25T00:00:00-0400\tPU:C4HFEACXX140625.4.TCGGAATG-TATGGTTC\TLB:Pond-350482\tPI:0\tSM:4142420025\tCN:BI\tPL:illumina
        rg="../input/fastqs/{sample}_{rg}.txt",
        # this is not actually used by the
        # wrapper (must be specified as params.index) this is here to ensure the
        # reference genome actually exists
        genome=reference_data["fasta"],
    output:
        temp("../scratch/{sample}/{sample}_{rg}.bam"),
    threads: 8
    params:
        extra=bwa_options_paired_end,
        sorting="samtools",
        index=reference_data["fasta"],
    resources:
        walltime=partial(increment, base=16, step=8),
    log:
        "../logs/bwa/{sample}_{rg}.log",
    wrapper:
        "0.78.0/bio/bwa/mem"


rule index_read_group:
    """
    Create the index for the read group BAM (needed for somalier).
    http://www.htslib.org/doc/samtools-index.html
    """
    input:
        "../scratch/{sample}/{sample}_{rg}.bam",
    output:
        temp("../scratch/{sample}/{sample}_{rg}.bam.bai"),
    resources:
        walltime=partial(increment, base=1, step=3),
    group:
        "somalier_rg"
    wrapper:
        "0.78.0/bio/samtools/index"


rule verifybamid_read_group:
    """
    Run verifybamid2 on the read group BAM to check for intraspecies
    contamination prior to merging.
    https://github.com/Griffan/VerifyBamID
    """
    input:
        bam="../scratch/{sample}/{sample}_{rg}.bam",
        bam_index="../scratch/{sample}/{sample}_{rg}.bam.bai",
        ref=reference_data["fasta"],
    output:
        selfsm="../scratch/{sample}/{sample}_{rg}.selfSM",
        ancestry="../scratch/{sample}/{sample}_{rg}.ancestry",
    params:
        # extracts ending numbers, e.g. 38 from hg38 or 37 from grch37
        genome_build=re.search(r"(\d+)$", genome_build).group(),
    log:
        "../logs/verifybamid/{sample}_{rg}.log",
    wrapper:
        "https://raw.githubusercontent.com/brcopeland/snakemake-wrappers/verifybamid2/bio/verifybamid/verifybamid2"


checkpoint check_rgs_contamination:
    """
    After running verifybamid2 on each read group BAM, check that the
    contamination from another human is sufficiently low.
    """
    input:
        # list of all .selfSM files for the given sample, one per read group.
        lambda wildcards: expand(
            f"../scratch/{wildcards['sample']}/{wildcards['sample']}_{{rg}}.selfSM",
            rg=aggregate_read_groups(wildcards),
        ),
    output:
        touch("../scratch/{sample}/check_contamination_rgs.done"),
    params:
        max_contamination=config["max_contamination"],
        failure_file="../scratch/{sample}/check_contamination_rgs.failure",
        email=config["email"],
    resources:
        walltime=0,
    log:
        "../logs/check_contamination/{sample}_rgs.log",
    script:
        "scripts/check_contamination.py"


rule somalier_extract_read_group:
    """
    Extract the information somalier needs to verify relatedness of all BAMs for
    the individual prior to merging.
    https://github.com/brentp/somalier
    """
    input:
        somalier="../bin/somalier",
        sites=f"../resources/sites.{config['genome_build']}.vcf.gz",
        genome=reference_data["fasta"],
        bam="../scratch/{sample}/{sample}_{rg}.bam",
        bam_index="../scratch/{sample}/{sample}_{rg}.bam.bai",
    output:
        temp("../scratch/{sample}/{sample}_{rg}.somalier"),
    params:
        # we prepend the read group and underscore to ensure somalier gives each
        # read group a distinct id
        sample_prefix=lambda wildcards: f"{wildcards['rg']}_",
    resources:
        walltime=0,
    group:
        "somalier_rg"
    shell:
        "t=$(mktemp -d) && {input.somalier} extract -d $t --sites {input.sites} "
        "-f {input.genome} --sample-prefix {params.sample_prefix} {input.bam} && "
        "mv $t/{wildcards.sample}.somalier {output} && rmdir $t "


rule somalier_relate_read_groups:
    """
    Do pairwise relatedness tests for all read group BAMs with somalier.
    https://github.com/brentp/somalier
    """
    input:
        # list of all data from "somalier extract" run on each read group for a sample
        somalier="../bin/somalier",
        somalier_fns=lambda wildcards: expand(
            f"../scratch/{wildcards['sample']}/{wildcards['sample']}_{{rg}}.somalier",
            rg=aggregate_read_groups(wildcards),
        ),
    output:
        "../output/qc/{sample}/somalier.pairs.tsv",
    params:
        # necessary to run command in a different directory so we get the full
        # paths to the input files
        input_paths=lambda wildcards, input: [
            os.path.abspath(input_path) for input_path in input["somalier_fns"]
        ],
        output_directory=get_output_directory,
    resources:
        walltime=0,
    shell:
        "cd {params.output_directory} && {input.somalier} relate {params.input_paths}"


checkpoint check_read_group_relatedness:
    """
    After running somalier relate on all read groups for the sample, confirm
    that each read group comes from the same person.
    https://github.com/brentp/somalier
    """
    input:
        "../output/qc/{sample}/somalier.pairs.tsv",
    output:
        touch("../scratch/{sample}/check_rg_relatedness.done"),
    params:
        min_relatedness=config["min_relatedness"],
        failure_file="../scratch/{sample}/check_rg_relatedness.failure",
        email=config["email"],
    resources:
        walltime=0,
    log:
        "../logs/check_rg_relatedness/{sample}.log",
    script:
        "scripts/check_rg_relatedness.py"


rule mark_duplicates:
    """
    Mark PCR duplicates and merge separate read group BAMs.
    Note that we have preserved read group information for downstream processing.
        (https://gatk.broadinstitute.org/hc/en-us/articles/360035889471-How-should-I-pre-process-data-from-multiplexed-sequencing-and-multi-library-designs-)
        https://gatk.broadinstitute.org/hc/en-us/articles/360037052812-MarkDuplicates-Picard-
    """
    input:
        aggregate_read_groups_markduplicates,
    output:
        bam=temp("../scratch/{sample}/{sample}_markdup.bam"),
        metrics="../output/qc/mark_duplicates/{sample}.txt",
    params:
        java_opts=get_java_temp_directory,
    resources:
        mem_mb=3000,
        walltime=partial(increment, base=16, step=8),
    log:
        "../logs/mark_duplicates/{sample}.log",
    wrapper:
        "0.78.0/bio/picard/markduplicates"


rule base_recalibration:
    """
    Generate a model for read base quality score recalibration.
    https://gatk.broadinstitute.org/hc/en-us/articles/360037593511-BaseRecalibrator
    """
    input:
        bam="../scratch/{sample}/{sample}_markdup.bam",
        dict=reference_data["fasta_dict"],
        ref=reference_data["fasta"],
        known=[
            reference_data["dbsnp"],
            reference_data["indel_mills"],
            reference_data["indel_known"],
        ],
    output:
        recal_table="../output/recal_tables/{sample}_recal.table",
    params:
        java_opts=get_java_temp_directory,
    resources:
        mem_mb=3000,
        walltime=partial(increment, base=8, step=8),
    log:
        "../logs/base_recalibration/{sample}.log",
    wrapper:
        "0.78.0/bio/gatk/baserecalibrator"


rule apply_base_recalibration:
    """
    Apply the model for read base quality score recalibration.
    https://gatk.broadinstitute.org/hc/en-us/articles/360037225212-ApplyBQSR
    """
    input:
        bam="../scratch/{sample}/{sample}_markdup.bam",
        dict=reference_data["fasta_dict"],
        recal_table="../output/recal_tables/{sample}_recal.table",
        ref=reference_data["fasta"],
    output:
        bam="../output/bams/{sample}.bam",
    params:
        java_opts=get_java_temp_directory,
    resources:
        mem_mb=3000,
        walltime=partial(increment, base=8, step=8),
    log:
        "../logs/apply_base_recalibration/{sample}.log",
    wrapper:
        "0.78.0/bio/gatk/applybqsr"


rule collectinsertsizemetrics:
    """
    Analyzes the distribution of insert sizes from the final BAM and produces a
    histogram summarizing this information.
    https://gatk.broadinstitute.org/hc/en-us/articles/360037055772-CollectInsertSizeMetrics-Picard-
    """
    input:
        bam="../output/bams/{sample}.bam",
    output:
        txt="../output/qc/collectinsertsizemetrics/{sample}.txt",
        pdf="../output/qc/collectinsertsizemetrics/{sample}.pdf",
    params:
        java_opts=get_java_temp_directory,
    resources:
        mem_mb=3000,
        walltime=partial(increment, base=8, step=8),
    log:
        "../logs/collectinsertsizemetrics/{sample}.bam",
    wrapper:
        "0.78.0/bio/picard/collectinsertsizemetrics"


rule verifybamid:
    """
    Run verifybamid2 on the final BAM to check for intraspecies contamination.
    https://github.com/Griffan/VerifyBamID
    """
    input:
        bam="../output/bams/{sample}.bam",
        ref=reference_data["fasta"],
    output:
        selfsm="../output/qc/verifybamid/{sample}.selfSM",
        ancestry="../output/qc/verifybamid/{sample}.ancestry",
    params:
        # extracts ending numbers, e.g. 38 from hg38 or 37 from grch37
        genome_build=re.search(r"(\d+)$", genome_build).group(),
    resources:
        walltime=partial(increment, base=4, step=8),
    log:
        "../logs/verifybamid/{sample}.log",
    wrapper:
        "https://raw.githubusercontent.com/brcopeland/snakemake-wrappers/verifybamid2/bio/verifybamid/verifybamid2"


checkpoint check_contamination:
    """
    After running verifybamid2 on the final BAM, check that the
    contamination from another human is sufficiently low.
    """
    input:
        "../output/qc/verifybamid/{sample}.selfSM",
    output:
        touch("../scratch/{sample}/check_contamination.done"),
    params:
        max_contamination=config["max_contamination"],
        failure_file="../scratch/{sample}/check_contamination.failure",
        email=config["email"],
    resources:
        walltime=0,
    log:
        "../logs/check_contamination/{sample}.log",
    script:
        "scripts/check_contamination.py"


rule mosdepth:
    """
    Get average read depth and number of bases reaching the specified thresholds
    with mosdepth.
    https://github.com/brentp/mosdepth
    """
    input:
        bam="../output/bams/{sample}.bam",
        bed=reference_data[config["sequencing_type"]]["bed"],
    output:
        multiext(
            "../output/qc/mosdepth/{sample}",
            ".thresholds.bed.gz",
            ".regions.bed.gz",
            ".mosdepth.region.dist.txt",
        ),
        summary="../output/qc/mosdepth/{sample}.mosdepth.summary.txt",
    params:
        thresholds=",".join([str(t) for t in config["mosdepth"]["thresholds"]]),
        extra=get_options("mosdepth", ignore=["thresholds"]),
    resources:
        walltime=partial(increment, base=1, step=3),
    log:
        "../logs/mosdepth/{sample}.log",
    wrapper:
        "0.78.0/bio/mosdepth"


checkpoint create_interval_files:
    """
    Create interval files for variant calling parallelization.
    This is adjusted for WES/WGS due to the different magnitude of data involved.
    """
    input:
        reference_data["fasta_dict"],
    output:
        touch("../scratch/create_interval_files.done"),
    params:
        megabases=config["create_interval_files"][config["sequencing_type"]],
        use_MT_flag="--use-MT" if genome_build == "grch37" else "",
        no_chr_prefix_flag="--no-chr-prefix" if genome_build == "grch37" else "",
        output_directory="../resources/intervals",
    resources:
        walltime=0,
    shell:
        "scripts/make_interval_files.py --dict {input} "
        "--megabases {params.megabases} {params.use_MT_flag} "
        "{params.no_chr_prefix_flag} --output-directory {params.output_directory}"


rule haplotype_caller:
    """
    Discover and assign initial genotypes in the given interval.
    https://gatk.broadinstitute.org/hc/en-us/articles/360037225632-HaplotypeCaller
    """
    input:
        bam=get_final_bam,
        ref=reference_data["fasta"],
        interval_file="../resources/intervals/intervalfile_{group}.list",
    output:
        gvcf="../scratch/{sample}/gvcfs/{sample}_{group}.g.vcf.gz",
    params:
        java_opts=get_java_temp_directory,
        extra=lambda wildcards, input: get_options(
            "haplotype_caller", extra=[("intervals", input.interval_file)]
        ),
    resources:
        mem_mb=3000,
        walltime=partial(increment, base=8, step=8),
    log:
        "../logs/haplotype_caller/{sample}_{group}.log",
    wrapper:
        "0.78.0/bio/gatk/haplotypecaller"


rule merge_gvcfs:
    """
    Combine all GVCFs for the sample.  This is primarily for archival purposes,
    as for throughput reasons we run joint-genotyping jobs on intervals of the genome,
    not the whole genome.
    https://broadinstitute.github.io/picard/command-line-overview.html#MergeVcfs
    """
    input:
        vcfs=lambda wildcards: expand(
            "../scratch/{{sample}}/gvcfs/{{sample}}_{group}.g.vcf.gz",
            group=get_intervals(),
        ),
    output:
        "../output/gvcfs/{sample}.g.vcf.gz",
    params:
        java_opts=get_java_temp_directory,
    resources:
        mem_mb=3000,
        mail="ae",
        walltime=partial(increment, base=1, step=3),
    log:
        "../logs/merge_gvcfs/{sample}.log",
    wrapper:
        "0.78.0/bio/picard/mergevcfs"


rule genomics_db_import:
    """
    Import into a GenomicsDB data store all GVCFs on the given interval for a set of samples.
    N.B. Everything previously was performed on single samples; this is where
    joint calling begins.
    https://gatk.broadinstitute.org/hc/en-us/articles/360037592851-GenomicsDBImport
    """
    input:
        gvcfs=expand(
            "../scratch/{sample}/gvcfs/{sample}_{{group}}.g.vcf.gz",
            sample=get_samples(),
        ),
    output:
        db=directory("../scratch/genomics_db_{group}"),
    params:
        java_opts=get_java_temp_directory,
        extra="--tmp-dir $TMPDIR",
        intervals="../resources/intervals/intervalfile_{group}.list",
    resources:
        walltime=partial(increment, base=8, step=8),
    log:
        "../logs/genomics_db_import/{group}.log",
    group:
        "genotype"
    wrapper:
        "0.78.0/bio/gatk/genomicsdbimport"


rule genotype_gvcfs:
    """
    Perform the actual joint calling on the created GenomicsDB data store.
    https://gatk.broadinstitute.org/hc/en-us/articles/360037594731-GenotypeGVCFs
    """
    input:
        genomicsdb="../scratch/genomics_db_{group}",
        ref=reference_data["fasta"],
        interval_file="../resources/intervals/intervalfile_{group}.list",
        known=reference_data["dbsnp"],
    output:
        vcf=temp("../scratch/vcfs/{group}.vcf"),
    params:
        java_opts=get_java_temp_directory,
        extra=get_options("genotype_gvcfs"),
    resources:
        walltime=partial(increment, base=8, step=8),
    log:
        "../logs/genotype_gvcfs/{group}.log",
    group:
        "genotype"
    wrapper:
        "0.78.0/bio/gatk/genotypegvcfs"


rule merge_vcfs:
    """
    Merge the VCFs created so as to create the VQSR model on all of the data.
    Previously we applied VQSR to the separate intervals independently,
        but this appears to be an unnecessary optimization.
    https://gatk.broadinstitute.org/hc/en-us/articles/360037594511-VariantRecalibrator
    https://broadinstitute.github.io/picard/command-line-overview.html#MergeVcfs
    """
    input:
        vcfs=lambda wildcards: expand(
            "../scratch/vcfs/{group}.vcf", group=get_intervals()
        ),
    output:
        temp("../scratch/vcfs/merged.vcf.gz"),
    params:
        java_opts=get_java_temp_directory,
    resources:
        mem_mb=3000,
        mail="ae",
        walltime=partial(increment, base=1, step=3),
    log:
        "../logs/merge_vcfs/merged_for_vqsr.log",
    wrapper:
        "0.78.0/bio/picard/mergevcfs"


rule snp_recalibration:
    """
    Build a model for reassigning SNV quality scores.
    https://gatk.broadinstitute.org/hc/en-us/articles/360035535932-Germline-short-variant-discovery-SNPs-Indels-
    https://gatk.broadinstitute.org/hc/en-us/articles/360037594511-VariantRecalibrator
    """
    input:
        vcf="../scratch/vcfs/merged.vcf.gz",
        ref=reference_data["fasta"],
        hapmap=reference_data["hapmap"],
        omni=reference_data["omni"],
        onekg=reference_data["onekg"],
        dbsnp=reference_data["dbsnp"],
    output:
        vcf=temp("../scratch/recal/snps.recal"),
        tranches=temp("../scratch/recal/snps.tranches"),
    params:
        mode="SNP",
        java_opts=get_java_temp_directory,
        extra=get_options("snp_recalibration", ignore=["resources", "annotation"]),
        resources=config["snp_recalibration"]["resources"],
        annotation=config["snp_recalibration"]["annotation"],
    resources:
        walltime=partial(increment, base=8, step=8),
    log:
        "../logs/snp_recalibration/log",
    wrapper:
        "0.78.0/bio/gatk/variantrecalibrator"


rule apply_snp_recalibration:
    """
    Apply the model for reassigning SNV quality scores.
    https://gatk.broadinstitute.org/hc/en-us/articles/360035535932-Germline-short-variant-discovery-SNPs-Indels-
    https://gatk.broadinstitute.org/hc/en-us/articles/360037226332-ApplyVQSR
    """
    input:
        vcf="../scratch/vcfs/merged.vcf.gz",
        recal="../scratch/recal/snps.recal",
        tranches="../scratch/recal/snps.tranches",
        ref=reference_data["fasta"],
    output:
        vcf=temp("../scratch/vcfs/recal_snp.vcf"),
    params:
        java_opts=get_java_temp_directory,
        mode="SNP",
        extra=get_options("apply_snp_recalibration"),
    resources:
        walltime=partial(increment, base=8, step=8),
    log:
        "../logs/apply_snp_recalibration/log",
    group:
        "apply_vqsr"
    wrapper:
        "0.78.0/bio/gatk/applyvqsr"


rule indel_recalibration:
    """
    Build a model for reassigning indel quality scores.
    https://gatk.broadinstitute.org/hc/en-us/articles/360035535932-Germline-short-variant-discovery-SNPs-Indels-
    https://gatk.broadinstitute.org/hc/en-us/articles/360037594511-VariantRecalibrator
    """
    input:
        vcf="../scratch/vcfs/merged.vcf.gz",
        ref=reference_data["fasta"],
        mills=reference_data["indel_mills"],
        axiom=reference_data["axiom"],
        dbsnp=reference_data["dbsnp"],
    output:
        vcf=temp("../scratch/recal/indels.recal"),
        tranches=temp("../scratch/recal/indels.tranches"),
    params:
        mode="INDEL",
        java_opts=get_java_temp_directory,
        extra=get_options("indel_recalibration", ignore=["resources", "annotation"]),
        resources=config["indel_recalibration"]["resources"],
        annotation=config["indel_recalibration"]["annotation"],
    resources:
        walltime=partial(increment, base=8, step=8),
    log:
        "../logs/indel_recalibration/log",
    wrapper:
        "0.78.0/bio/gatk/variantrecalibrator"


rule apply_indel_recalibration:
    """
    Apply the model for reassigning indel quality scores.
    https://gatk.broadinstitute.org/hc/en-us/articles/360035535932-Germline-short-variant-discovery-SNPs-Indels-
    https://gatk.broadinstitute.org/hc/en-us/articles/360037226332-ApplyVQSR
    """
    input:
        vcf="../scratch/vcfs/recal_snp.vcf",
        recal="../scratch/recal/indels.recal",
        tranches="../scratch/recal/indels.tranches",
        ref=reference_data["fasta"],
    output:
        vcf="../output/joint_vcf/final.vcf.gz",
    params:
        omava_opts=get_java_temp_directory,
        mode="INDEL",
        extra=get_options("apply_indel_recalibration"),
    resources:
        walltime=partial(increment, base=8, step=8),
    log:
        "../logs/apply_indel_recalibration/log",
    group:
        "apply_vqsr"
    wrapper:
        "0.78.0/bio/gatk/applyvqsr"


rule somalier_extract_joint_vcf:
    """
    Extract the information somalier needs to check relatedness of all samples
    from the joint genotyped VCF.
    https://github.com/brentp/somalier
    """
    input:
        somalier="../bin/somalier",
        sites=f"../resources/sites.{config['genome_build']}.vcf.gz",
        genome=reference_data["fasta"],
        vcf="../output/joint_vcf/final.vcf.gz",
    output:
        expand("../scratch/extracted/{sample}.somalier", sample=get_samples()),
    params:
        output_directory=get_output_directory,
    resources:
        walltime=partial(increment, base=1, step=3),
    group:
        "somalier_vcf"
    shell:
        "{input.somalier} extract -d {params.output_directory} "
        "--sites {input.sites} -f {input.genome} {input.vcf}"


rule somalier_relate_samples:
    """
    Do pairwise relatedness tests for all samples with somalier.
    Uses PED_FILE if present.
    https://github.com/brentp/somalier
    """
    input:
        somalier="../bin/somalier",
        somalier_fns=expand(
            "../scratch/extracted/{sample}.somalier", sample=get_samples()
        ),
    output:
        "../output/qc/somalier/somalier.pairs.tsv",
        "../output/qc/somalier/somalier.samples.tsv",
    params:
        # necessary to run command in a different directory so we get the full
        # paths to the input files
        input_paths=lambda wildcards, input: [
            os.path.abspath(input_path) for input_path in input["somalier_fns"]
        ],
        output_directory=get_output_directory,
        ped=lambda wildcards: (
            f"--ped {os.path.abspath(PED_FILE)}" if os.path.isfile(PED_FILE) else ""
        ),
    resources:
        walltime=0,
    shell:
        "cd {params.output_directory} && {input.somalier} relate {params.ped} {params.input_paths}"


rule predict_sex:
    """
    Check the ratio of heterozygous:homozygous calls on the X chromosome as
    reported by somalier.
    NYI: prediction of sex chromosome aneuploidies
    """
    input:
        "../output/qc/somalier/somalier.samples.tsv",
    output:
        "../output/qc/somalier/samples_with_predicted_sex.ped",
    params:
        sex_check_threshold=config["sex_check_threshold"],
    resources:
        walltime=0,
    run:
        with open(input[0]) as samples_fh, open(output[0], "w") as ped_fh:
            header = samples_fh.readline().strip().split("\t")
            for line in samples_fh:
                d = dict(zip(header, line.strip().split("\t")))
                het_hom_ratio = int(d["X_het"]) / (
                    int(d["X_hom_ref"]) + int(d["X_hom_alt"])
                )
                if het_hom_ratio < params.sex_check_threshold:
                    d["predicted_sex"] = 1
                else:
                    d["predicted_sex"] = 2
                ped_fh.write(
                    "{#family_id}\t{sample_id}\t{paternal_id}\t{maternal_id}"
                    "\t{predicted_sex}\t{phenotype}\n".format(**d)
                )


rule check_sex:
    """
    Check the predicted sex from the predict_sex classifier against the original
    PED file.
    """
    input:
        predicted_ped="../output/qc/somalier/samples_with_predicted_sex.ped",
        original_ped=PED_FILE,
    output:
        touch("../scratch/check_sex.done"),
    params:
        failure_file="../scratch/check_sex.failure",
        email=config["email"],
    resources:
        walltime=0,
    log:
        "../logs/check_sex.log",
    script:
        "scripts/check_sex.py"


rule check_relatedness:
    """
    Check pairwise relatedness from somalier relate.  This assumes a PED file
    has been provided and we check for relatedness in the expected range within
    families and no cryptic relatedness.
    """
    input:
        pairs="../output/qc/somalier/somalier.pairs.tsv",
        ped=PED_FILE,
    output:
        touch("../scratch/check_relatedness.done"),
    params:
        cryptic_relatedness_threshold=config["cryptic_relatedness_threshold"],
        intra_family_cryptic_relatedness_threshold=config[
            "intra_family_cryptic_relatedness_threshold"
        ],
        first_degree_relatedness_min=config["first_degree_relatedness"]["min"],
        first_degree_relatedness_max=config["first_degree_relatedness"]["max"],
        failure_file="../scratch/check_relatedness.failure",
        email=config["email"],
    resources:
        walltime=0,
    log:
        "../logs/check_relatedness.log",
    script:
        "scripts/check_relatedness.py"


rule copy_program_to_scratch:
    """
    Copy a program from projects storage to scratch for use in the pipeline.
    """
    input:
        f"{config['resources_repository']}/bin/{{program}}",
    output:
        "../bin/{program}",
    params:
        output_directory=get_output_directory,
    resources:
        walltime=1,
    shell:
        "rsync -L {input} {params.output_directory}"


rule copy_resource_to_scratch:
    """
    Copy a file from projects storage to scratch for use in the pipeline.
    """
    input:
        f"{config['resources_repository']}/{genome_build}/{{resource}}",
    output:
        "../resources/{resource}",
    params:
        output_directory=get_output_directory,
    resources:
        walltime=1,
    shell:
        "rsync -L {input}* {params.output_directory}"
